<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Kernel Extraction via Voted Risk Minimization | FE 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Kernel Extraction via Voted Risk Minimization">

  <meta name="citation_author" content="Cortes, Corinna">

  <meta name="citation_author" content="Goyal, Prasoon">

  <meta name="citation_author" content="Kuznetsov, Vitaly">

  <meta name="citation_author" content="Mohri, Mehryar">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 1st International Workshop on “Feature Extraction: Modern Questions and Challenges”, NIPS">
<meta name="citation_firstpage" content="72">
<meta name="citation_lastpage" content="89">
<meta name="citation_pdf_url" content="cortes15a.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Kernel Extraction via Voted Risk Minimization</h1>

	<div id="authors">
	
		Corinna Cortes,
	
		Prasoon Goyal,
	
		Vitaly Kuznetsov,
	
		Mehryar Mohri
	<br />
	</div>
	<div id="info">
		Proceedings of The 1st International Workshop on “Feature Extraction: Modern Questions and Challenges”, NIPS,
		pp. 72–89, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		This paper studies a new framework for learning a predictor in the presence of multiple kernel functions where the learner selects or extracts several kernel functions from potentially complex families and finds an accurate predictor defined in terms of these functions. We present an algorithm, Voted Kernel Regularization, that provides the flexibility of using very complex kernel functions such as predictors based on high-degree polynomial kernels or narrow Gaussian kernels, while benefitting from strong learning guarantees. We show that our algorithm benefits from strong learning guarantees suggesting a new regularization penalty depending on the Rademacher complexities of the families of kernel functions used. Our algorithm admits several other favorable properties: its optimization problem is convex, it allows for learning with non-PDS kernels, and the solutions are highly sparse, resulting in improved classification speed and memory requirements. We report the results of some preliminary experiments comparing the performance of our algorithm to several baselines.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="cortes15a.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
