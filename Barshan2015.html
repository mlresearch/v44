<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Stage-wise Training: An Improved Feature Learning Strategy for Deep Models | FE 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Stage-wise Training: An Improved Feature Learning Strategy for Deep Models">

  <meta name="citation_author" content="Barshan, Elnaz">

  <meta name="citation_author" content="Fieguth, Paul">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 1st International Workshop on “Feature Extraction: Modern Questions and Challenges”, NIPS">
<meta name="citation_firstpage" content="49">
<meta name="citation_lastpage" content="59">
<meta name="citation_pdf_url" content="Barshan2015.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Stage-wise Training: An Improved Feature Learning Strategy for Deep Models</h1>

	<div id="authors">
	
		Elnaz Barshan,
	
		Paul Fieguth
	<br />
	</div>
	<div id="info">
		Proceedings of The 1st International Workshop on “Feature Extraction: Modern Questions and Challenges”, NIPS,
		pp. 49–59, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Deep neural networks currently stand at the state of the art for many machine learning applications, yet there still remain limitations in the training of such networks because of their very high parameter dimensionality. In this paper we show that network training performance can be improved using a stage-wise learning strategy, in which the learning process is broken down into a number of related sub-tasks that are completed stage-by-stage. The idea is to inject the information to the network <em>gradually</em> so that in the early stages of training the “coarse-scale” properties of the data are captured while the “finer-scale” characteristics are learned in later stages. Moreover, the solution found in each stage serves as a prior to the next stage, which produces a regularization effect and enhances the generalization of the learned representations. We show that decoupling the classifier layer from the feature extraction layers of the network is necessary, as it alleviates the diffusion of gradient and over-fitting problems. Experimental results in the context of image classification support these claims.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Barshan2015.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
