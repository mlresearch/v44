---
title: Minimum description length (MDL) regularization for online learning
abstract: An approach inspired by the \emphMinimum Description Length (MDL) principle
  is proposed for adaptively selecting features during online learning based on their
  usefulness in improving the objective. The approach eliminates noisy or useless
  features from the optimization process, leading to improved loss. Several algorithmic
  variations on the approach are presented.  They are based on using a Bayesian mixture
  in each of the dimensions of the feature space.  By utilizing the MDL principle,
  the mixture reduces the dimensionality of the feature space to its subspace with
  the lowest loss.  Bounds on the loss, derived, show that the loss for that subspace
  is essentially achieved. The approach can be tuned for trading off between model
  size and the loss incurred. Empirical results on large scale real-world systems
  demonstrate how it improves such tradeoffs. Huge model size reductions can be achieved
  with no loss in performance relative to standard techniques, while moderate loss
  improvements (translating to large regret improvements) are achieved with moderate
  size reductions. The results also demonstrate that overfitting is eliminated by
  this approach.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shamir15
month: 0
tex_title: Minimum description length ({MDL}) regularization for online learning
firstpage: 260
lastpage: 276
page: 260-276
sections: 
author:
- given: Gil I.
  family: Shamir
date: 2015-12-08
address: Montreal, Canada
publisher: PMLR
container-title: 'Proceedings of the 1st International Workshop on Feature Extraction:
  Modern Questions and Challenges at NIPS 2015'
volume: '44'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 12
  - 8
pdf: http://proceedings.mlr.press/v44/shamir15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
