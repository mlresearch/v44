---
title: A Dimension-Independent Generalization Bound for Kernel Supervised Principal
  Component Analysis
abstract: Kernel supervised principal component analysis (KSPCA) is a computationally
  efficient supervised feature extraction method that can learn non-linear transformations.
  We start the study of the statistical properties of KSPCA, providing the first bound
  on its sample complexity. This bound is dimension-independent, which justifies the
  good performance of KSPCA on high-dimensional data. Another observation is that
  in the kernelized version, the number of parameters of KSPCA grows linearly with
  the sample size. While this potentially increases the risk of over-fitting, KSPCA
  works well in practice. In this work, we justify this compelling characteristic
  of KSPCA by providing a guarantee indicating that KSPCA generalizes well even when
  the number of parameters is large, as long as they have small norms.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Ashtiani2015
month: 0
firstpage: 19
lastpage: 29
page: 19-29
sections: 
author:
- given: Hassan
  family: Ashtiani
- given: Ali
  family: Ghodsi
date: 2015-12-08
address: Montreal, Canada
publisher: PMLR
container-title: 'Proceedings of the 1st International Workshop on Feature Extraction:
  Modern Questions and Challenges at NIPS 2015'
volume: '44'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 12
  - 8
pdf: http://proceedings.mlr.press/v44/Ashtiani2015.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
