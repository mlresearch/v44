---
title: Generalization Bounds for Supervised Dimensionality Reduction
abstract: We introduce and study the learning scenario of \emphsupervised dimensionality
  reduction, which couples dimensionality reduction and a subsequent supervised learning
  step.  We present new generalization bounds for this scenario based on a careful
  analysis of the empirical Rademacher complexity of the relevant hypothesis set.
  In particular, we show an upper bound on the Rademacher complexity that is in \widetilde
  O(\sqrt\Lambda_(r)/m), where m is the sample size and \Lambda_(r) the upper bound
  on the Ky-Fan r-norm of the operator that defines the dimensionality reduction projection.
  We give both upper and lower bound guarantees in terms of that Ky-Fan r-norm, which
  strongly justifies the definition of our hypothesis set. To the best of our knowledge,
  these are the first learning guarantees for the problem of supervised dimensionality
  reduction with a \emphlearned kernel-based mapping. Our analysis and learning guarantees
  further apply to several special cases, such as that of using a fixed kernel with
  supervised dimensionality reduction or that of unsupervised learning of a kernel
  for dimensionality reduction followed by a supervised learning algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mohri2015generalization
month: 0
firstpage: 226
lastpage: 241
page: 226-241
sections: 
author:
- given: Mehryar
  family: Mohri
- given: Afshin
  family: Rostamizadeh
- given: Dmitry
  family: Storcheus
date: 2015-12-08
address: Montreal, Canada
publisher: PMLR
container-title: 'Proceedings of the 1st International Workshop on Feature Extraction:
  Modern Questions and Challenges at NIPS 2015'
volume: '44'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 12
  - 8
pdf: http://proceedings.mlr.press/v44/mohri2015generalization/mohri2015generalization.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
