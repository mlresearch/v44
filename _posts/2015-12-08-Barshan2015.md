---
title: 'Stage-wise Training: An Improved Feature Learning Strategy for Deep Models'
abstract: Deep neural networks currently stand at the state of the art for many machine
  learning applications, yet there still remain limitations in the training of such
  networks because of their very high parameter dimensionality. In this paper we show
  that network training performance can be improved using a stage-wise learning strategy,
  in which the learning process is broken down into a number of related sub-tasks
  that are completed stage-by-stage. The idea is to inject the information to the
  network \textitgradually so that in the early stages of training the “coarse-scale”
  properties of the data are captured while the “finer-scale” characteristics are
  learned in later stages. Moreover, the solution found in each stage serves as a
  prior to the next stage, which produces a regularization effect and enhances the
  generalization of the learned representations. We show that decoupling the classifier
  layer from the feature extraction layers of the network is necessary, as it alleviates
  the diffusion of gradient and over-fitting problems.  Experimental results in the
  context of image classification support these claims.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Barshan2015
month: 0
firstpage: 49
lastpage: 59
page: 49-59
sections: 
author:
- given: Elnaz
  family: Barshan
- given: Paul
  family: Fieguth
date: 2015-12-08
address: Montreal, Canada
publisher: PMLR
container-title: 'Proceedings of the 1st International Workshop on Feature Extraction:
  Modern Questions and Challenges at NIPS 2015'
volume: '44'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 12
  - 8
pdf: http://proceedings.mlr.press/v44/Barshan2015.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
